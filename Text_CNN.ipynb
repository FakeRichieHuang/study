{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据读取与预处理\n",
    "#本程序使用Kim2014的TextCNN对Malo2014的FinancialPhraseBank数据集进行分类\n",
    "with open('/root/study/Data/FinancialPhraseBank/Sentences_75Agree.txt','r')as f:\n",
    "    FPB_75_in = f.read() #Malo的数据特色，分为不同置信度的样本\n",
    "with open('/root/study/Data/FinancialPhraseBank/Sentences_AllAgree.txt','r')as f:\n",
    "    FPB_100_in = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral\\nWith the new production plant the company would increase its cap'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FPB_75_in[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FPB_75_lines = FPB_75_in.split('\\n')\n",
    "FPB_75_lines[0].split('@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "FPB_75_data = []\n",
    "FPB_75_label = []\n",
    "tags = {'positive':2, 'neutral':1, 'negative':0} #标签映射\n",
    "for line in FPB_75_lines[:-1]:\n",
    "    line = line.lower() #小写化\n",
    "    data_line = line.split('@')[0] #数据部分\n",
    "    FPB_75_label.append(tags[line.split('@')[1]]) #标签部分\n",
    "    data_line = ''.join([c for c in data_line if c not in punctuation])#去除标点符号\n",
    "    data_l = data_line.split(' ') #分词\n",
    "    data_l = [c for c in data_l if len(c)>0] #Malo这份数据的特别之处，标点前有空格，去除空格\n",
    "    FPB_75_data.append(data_l) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289a092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['according', 'to', 'gran', 'the', 'company', 'has', 'no', 'plans', 'to', 'move', 'all', 'production', 'to', 'russia', 'although', 'that', 'is', 'where', 'the', 'company', 'is', 'growing'] \n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "print(FPB_75_data[0],'\\n',FPB_75_label[0]) #查看数据和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a022963",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPB_100_lines = FPB_100_in.split('\\n')\n",
    "FPB_100_data = []\n",
    "FPB_100_label = []\n",
    "tags = {'positive':2, 'neutral':1, 'negative':0} #标签映射\n",
    "for line in FPB_100_lines[:-1]:\n",
    "    line = line.lower() #小写化\n",
    "    data_line = line.split('@')[0] #数据部分\n",
    "    FPB_100_label.append(tags[line.split('@')[1]]) #标签部分\n",
    "    data_line = ''.join([c for c in data_line if c not in punctuation])#去除标点符号\n",
    "    data_l = data_line.split(' ') #分词\n",
    "    data_l = [c for c in data_l if len(c)>0] #Malo这份数据的特别之处，标点前有空格，去除空格\n",
    "    FPB_100_data.append(data_l) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36c1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPB_data_raw = FPB_75_data + FPB_100_data\n",
    "FPB_label_raw = FPB_75_label + FPB_100_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c376619d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5717.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.027287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.876611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  5717.000000\n",
       "mean     20.027287\n",
       "std       8.876611\n",
       "min       1.000000\n",
       "25%      13.000000\n",
       "50%      19.000000\n",
       "75%      26.000000\n",
       "max      52.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP_len = []\n",
    "for li in FPB_data_raw:\n",
    "    FP_len.append(len(li))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(FP_len).describe() #查看数据长度分布，确定切断尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01df36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于是拼接的100一致和75一致，100在后面，避免这个影响要先random\n",
    "import random\n",
    "random.seed(42)\n",
    "raw_idx = list(range(len(FPB_data_raw)))\n",
    "random.shuffle(raw_idx)\n",
    "\n",
    "FPB_data = []\n",
    "FPB_label = []\n",
    "for idx in raw_idx:\n",
    "    FPB_data.append(FPB_data_raw[idx])\n",
    "    FPB_label.append(FPB_label_raw[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e04a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FPB_label_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef426c2a",
   "metadata": {},
   "source": [
    "加载预训练w2v模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6cfd7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18680"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embed_lookup = KeyedVectors.load_word2vec_format('/root/study/Data/model/GoogleNews-vectors-negative300-SLIM.bin',binary=True) #加载预训练w2v模型\n",
    "embed_lookup.key_to_index['hello'] #测试一下是否加载成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd70b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec_index(wordl):#本函数获得所有词的w2v的index\n",
    "    resl = []\n",
    "    for wi in wordl:\n",
    "        try:\n",
    "            idx = embed_lookup.key_to_index[wi] #获取词向量索引\n",
    "        except Exception:\n",
    "            idx = 0\n",
    "        resl.append(idx)\n",
    "    return resl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58ba117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPB_vec_index = list(map(get_vec_index,FPB_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9de1519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def padding_tokens(raw_vec,padding_len=60,sq_len=None):#对所有数据进行补齐，在本例中所有数据长度为60\n",
    "    tlen = len(raw_vec)\n",
    "    if sq_len is not None:\n",
    "        sq_len.append(tlen)\n",
    "    if tlen>=padding_len:\n",
    "        return raw_vec[:padding_len]\n",
    "    \n",
    "    for li in range(padding_len-tlen):\n",
    "        raw_vec.append(0)\n",
    "    return np.array(raw_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40a1fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_len = []\n",
    "FPB_vec_index_padding = list(map(lambda x: padding_tokens(x,padding_len=60,sq_len=sq_len),FPB_vec_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bf1d317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   796,    864,    563,      0,  17001,      0],\n",
       "       [   468,      4,    584, 178363,      8,      0],\n",
       "       [     0,   6638,      0,     10,      9,     54],\n",
       "       [     9,   1179,    130,    840,      0,    682],\n",
       "       [182879,    730,      0,      0,   1239,   6407],\n",
       "       [     9,     94,   1006,      3,      0,      0],\n",
       "       [     9,    429,      3,    249,      0,     14],\n",
       "       [     0,      1,      9,     32,      0,   1470],\n",
       "       [     0,      0,      0,      0,    864,     93],\n",
       "       [   172,    796,    864,      0,   5119,      1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array(FPB_vec_index_padding)\n",
    "labels = np.array(FPB_label)\n",
    "features[:10,:6] #查看前10条数据的向量表示，60维度的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c5538b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#标准化数据\n",
    "#切分训练集、测试集、验证集\n",
    "train_frac = 0.8\n",
    "valid_frac = 0.9\n",
    "\n",
    "train_idx = int(len(features)*train_frac)\n",
    "valid_idx = int(len(features)*valid_frac)\n",
    "\n",
    "train_x,valid_x,test_x = features[:train_idx],features[train_idx:valid_idx],features[valid_idx:]\n",
    "train_y,valid_y,test_y = labels[:train_idx],labels[train_idx:valid_idx],labels[valid_idx:]\n",
    "train_sq_len,valid_sq_len,test_sq_len = sq_len[:train_idx],sq_len[train_idx:valid_idx],sq_len[valid_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "949f9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#制作标准Tensor数据集、包含特征、标签\n",
    "train_dataset = TensorDataset(torch.tensor(train_x, dtype=torch.long), torch.tensor(train_y, dtype=torch.long))\n",
    "valid_dataset = TensorDataset(torch.tensor(valid_x, dtype=torch.long), torch.tensor(valid_y, dtype=torch.long))\n",
    "test_dataset = TensorDataset(torch.tensor(test_x, dtype=torch.long), torch.tensor(test_y, dtype=torch.long))\n",
    "#tensordataset可以把数据转化为Tensor形式，便于后续处理\n",
    "batch_size = 50\n",
    "\n",
    "#随机化与Batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d55d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存数据，以供后期使用\n",
    "torch.save({\"data\":train_dataset.tensors[0], \"labels\":train_dataset.tensors[1],\"lengths\":train_sq_len}, '/root/study/Data/Malo_train.pth')\n",
    "torch.save({\"data\":valid_dataset.tensors[0], \"labels\":valid_dataset.tensors[1],\"lengths\":valid_sq_len}, '/root/study/Data/Malo_valid.pth')\n",
    "torch.save({\"data\":test_dataset.tensors[0], \"labels\":test_dataset.tensors[1],\"lengths\":test_sq_len}, '/root/study/Data/Malo_test.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85fdd1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_x) #查看验证集的长度，应该是0.1*数据集长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b616ce62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集-Batch 0:\n",
      "Features: torch.Size([50, 60]), Labels: torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "#检查dataloader的输出\n",
    "for batch_idx,(x,y) in enumerate(train_loader):\n",
    "    print(f\"训练集-Batch {batch_idx}:\")\n",
    "    print(f\"Features: {x.shape}, Labels: {y.shape}\")\n",
    "    break #查看第一个批次的结果即可\n",
    "#x的shape是(batch_size, seq_len)，y的shape是(batch_size,)，即每个batch的特征和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03b68aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理完成，接下来可以进行模型构建和训练\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embed_model, vocab_size, output_size,embed_dim,\n",
    "                 num_filters=100, kernel_sizes=[3,4,5], embed_freeze=True,dropout=0.5):\n",
    "        super(TextCNN, self).__init__()\n",
    "        #设定参数\n",
    "        self.num_filters = num_filters #卷积核的数量\n",
    "        self.embed_dim = embed_dim #词向量会被映射为多少维\n",
    "\n",
    "        #  1.词嵌入层\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        # 加载预训练的权重\n",
    "        self.embed.weight = nn.Parameter(torch.from_numpy(embed_model.vectors))\n",
    "        # 冻结嵌入层的训练，设定为false则一起训练\n",
    "        if embed_freeze:\n",
    "            self.embed.weight.requires_grad = False\n",
    "        \n",
    "        #  2.卷积层\n",
    "        self.convs_1d = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embed_dim),padding = (k-2, 0)) for k in kernel_sizes\n",
    "        ])#并联多个二维卷积层，每个卷积核的宽度设定为embed_dim，高度为不同的k值\n",
    "        #  3.全连接层\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size)\n",
    "        #  4. dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def conv_and_pool(self,x,conv): #定义一个卷积和池化的函数\n",
    "        x = F.relu(conv(x)).squeeze(3) #卷积后激活,对于每个conv(一维卷积)，卷积后的结果第四个维度会是1，可以删掉squeeze）\n",
    "        #四个维度分别是batch_size,num_filters,sequence_length,1\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2) #最大池化，池化后结果的维度是(batch_size,num_filters)，即每个卷积核的最大值\n",
    "        #然后会删掉第三个维度\n",
    "        return x_max\n",
    "        #返回的是(batch_size, num_filters)，即每个卷积核的最大值\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 1.词嵌入\n",
    "        embeds = self.embed(x) #输入x x是一个batch的索引，维度是(batch_size, seq_len),seq_len是每个句子的长度\n",
    "        #输出embeds的维度是(batch_size, seq_len, embed_dim),嵌入向量每个元素的维度是embed_dim\n",
    "        embeds = embeds.unsqueeze(1) #增加一个维度，变为(batch_size, 1, seq_len, embed_dim)，1是单通道\n",
    "        #目的是为了适应卷积层的输入格式，卷积层需要四维输入(batch_size, channels, height, width)\n",
    "\n",
    "        # 2.卷积和池化\n",
    "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
    "        #self.convs_ld是一个包含多个2d卷积核的moudulelist，每个卷积核负责不同大小的窗口kernel_size\n",
    "        #对每个卷积核进行卷积和池化(通过self.conv_and_pool提取特征)，\n",
    "        #得到每个卷积核生成的形状为(batch_size, num_filters)的特征向量\n",
    "        #结果是一个列表，每个元素是(batch_size, num_filters)，即每个卷积核的最大值\n",
    "\n",
    "        # 3.拼接卷积结果\n",
    "        x = torch.cat(conv_results,1) #将所有卷积核的结果拼接在一起，变为(batch_size, num_filters * len(kernel_sizes))\n",
    "        #即每个卷积核的最大值拼接在一起\n",
    "\n",
    "        # 4. dropout\n",
    "        x = self.dropout(x) #对拼接后的结果进行dropout，防止过拟合\n",
    "\n",
    "        # 5.全连接层\n",
    "        x = self.fc(x) #将拼接后的结果输入全连接层，输出的维度是(batch_size, output_size)\n",
    "        #output_size是分类的数量\n",
    "        return F.log_softmax(x, dim=1) #使用log_softmax进行分类，dim=1表示对每一行进行softmax   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a4847fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练函数与测试函数仍然可以复用\n",
    "#定义一个训练函数\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    # args：一般是一个包含超参数的对象或命名空间（如 batch_size, learning_rate 等），用于控制训练过程。\n",
    "    # model：传入的神经网络模型，需要被训练。\n",
    "    # device：设备信息（如 cpu 或 cuda），决定模型和数据在哪个硬件上运行。\n",
    "    # train_loader：数据加载器，包含训练数据的批次，用于迭代访问每个 batch。\n",
    "    # optimizer：优化器，用于更新模型的参数（如 SGD、Adam）。\n",
    "    # epoch：当前的训练轮次编号，用于记录进度。\n",
    "    \n",
    "    model.train()  #将模型切换为训练模式。pytorch中有train和eval两个模式，一些机制如dropout Batchnorm仅在train模式中生效\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  #从train_loader中加载数据，data、target分别为数据和标签，batch_idx为批次索引从0开始\n",
    "        data, target = data.to(device), target.to(device)  #将数据和标签转移到指定的设备（CPU 或 GPU）。如果数据和标签不在相同设备上则会报错\n",
    "        optimizer.zero_grad()     #清空优化器中所有参数的梯度缓存。PyTorch 的默认行为是 梯度累积（每次调用 backward() 会将新梯度累加到现有的梯度上），因此需要在每次迭代开始前清空梯度。\n",
    "        output = model(data)      #使用模型对当前批次的输入 data 进行前向传播，得到预测结果 output\n",
    "        loss = F.nll_loss(output, target)  #计算预测值与真实标签之间的误差（损失）：F.nll_loss是 PyTorch 提供的负对数似然损失函数（Negative Log Likelihood Loss）\n",
    "        loss.backward()           #loss反向传播\n",
    "        optimizer.step()          #根据计算得到的梯度，利用优化器更新模型的参数\n",
    "\n",
    "#定义一个测试函数\n",
    "def test(model, device, test_loader):\n",
    "    # model：需要评估的神经网络模型。\n",
    "    # device：用于运行模型和数据的设备（如 CPU 或 GPU）。\n",
    "    # test_loader：数据加载器，提供测试数据集，通常不会打乱顺序。\n",
    "    \n",
    "    model.eval()  #将模型切换为eval模式\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  #禁用自动求导功能，节省资源，加速运行\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)    #将数据和标签转移到指定的设备（CPU 或 GPU）。如果数据和标签不在相同设备上则会报错\n",
    "            output = model(data)    #将数据输入模型，得到预测结果 output\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  #使用负对数似然损失（nll_loss）计算当前批次的损失，并将其加总\n",
    "            pred = output.argmax(dim=1, keepdim=True)   #从模型输出中选择每个样本中最大值对应的索引作为预测类别。\n",
    "                                                        #argmax(dim=1)：在每行（样本）的预测值中找到概率最大的类别索引。\n",
    "                                                        #keepdim=True：保持结果的维度一致，方便与标签进行比较。\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  #累计正确预测数量；pred.eq(target.view_as(pred))：比较预测值 pred 和真实值 target 是否相等，返回一个布尔张量。\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  #总测试损失除以测试集样本总数，得到平均损失。\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\" - Average Loss: {test_loss:.4f}\")\n",
    "    print(f\" - Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
    "          f\"({100. * correct / len(test_loader.dataset):.2f}%)\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90d7deeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "epochs = 6  #定义最大训练周期\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  #定义设备,CPU或者GPU（Cuda）\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abd059f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_words = []\n",
    "for word in embed_lookup.key_to_index:\n",
    "    pretrained_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b6a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embed): Embedding(299567, 300)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(pretrained_words)\n",
    "output_size = 3 \n",
    "embed_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "model = TextCNN(embed_lookup, vocab_size, output_size, embed_dim,\n",
    "                   num_filters, kernel_sizes).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7ef75a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adadelta(model.parameters()) #Adadelta 是一种自适应学习率优化算法，不需要显式设置全局学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e85cba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Evaluation Results:\n",
      " - Average Loss: 0.5813\n",
      " - Accuracy: 432/572 (75.52%)\n",
      "==================================================\n",
      "Epoch 1/6 - Trained in 0.88 seconds  - Test in 0.06 seconds.\n",
      "\n",
      "==================================================\n",
      "Evaluation Results:\n",
      " - Average Loss: 0.5267\n",
      " - Accuracy: 447/572 (78.15%)\n",
      "==================================================\n",
      "Epoch 2/6 - Trained in 0.50 seconds  - Test in 0.05 seconds.\n",
      "\n",
      "==================================================\n",
      "Evaluation Results:\n",
      " - Average Loss: 0.2978\n",
      " - Accuracy: 517/572 (90.38%)\n",
      "==================================================\n",
      "Epoch 3/6 - Trained in 0.50 seconds  - Test in 0.04 seconds.\n",
      "\n",
      "==================================================\n",
      "Evaluation Results:\n",
      " - Average Loss: 0.2366\n",
      " - Accuracy: 532/572 (93.01%)\n",
      "==================================================\n",
      "Epoch 4/6 - Trained in 0.50 seconds  - Test in 0.04 seconds.\n",
      "\n",
      "==================================================\n",
      "Evaluation Results:\n",
      " - Average Loss: 0.2047\n",
      " - Accuracy: 536/572 (93.71%)\n",
      "==================================================\n",
      "Epoch 5/6 - Trained in 0.50 seconds  - Test in 0.04 seconds.\n",
      "\n",
      "==================================================\n",
      "Evaluation Results:\n",
      " - Average Loss: 0.1918\n",
      " - Accuracy: 537/572 (93.88%)\n",
      "==================================================\n",
      "Epoch 6/6 - Trained in 0.50 seconds  - Test in 0.04 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "import time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()  #记录开始时间\n",
    "    train([], model, device, train_loader, optimizer, epoch)  #训练\n",
    "    train_time = time.time()\n",
    "    test(model, device, valid_loader)  #测试\n",
    "    test_time = time.time()\n",
    "    train_duration = train_time - start_time\n",
    "    test_duration = test_time - train_time\n",
    "    print(f\"Epoch {epoch}/{epochs} - Trained in {train_duration:.2f} seconds  - Test in {test_duration:.2f} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c941831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Evaluation Results:\n",
      " - Average Loss: 0.1918\n",
      " - Accuracy: 537/572 (93.88%)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "test(model, device, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
